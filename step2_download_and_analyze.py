#!/usr/bin/env python3
"""
Step 2: ìˆ˜ì§‘ëœ YouTube URL ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„
Step 1ì—ì„œ ìˆ˜ì§‘í•œ URLë“¤ì„ ì‹¤ì œë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì˜¤ë””ì˜¤ ë¶„ì„ ìˆ˜í–‰
"""

import os
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import argparse

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from app.services.audio_processor import audio_processor

class YouTubeAnalyzer:
    """YouTube ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.analysis_results = []
        self.failed_analyses = []
        self.output_dir = Path("data/analyzed")
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_collection(self, collection_file: str) -> List[Dict]:
        """Step 1ì—ì„œ ìˆ˜ì§‘í•œ URL ë°ì´í„° ë¡œë“œ"""
        
        file_path = Path("data/collected") / collection_file
        if not file_path.exists():
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ í™•ì¸
            file_path = Path(collection_file)
            if not file_path.exists():
                raise FileNotFoundError(f"ìˆ˜ì§‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {collection_file}")
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        videos = data.get("videos", [])
        metadata = data.get("metadata", {})
        
        logger.info(f"ğŸ“ ìˆ˜ì§‘ íŒŒì¼ ë¡œë“œ: {file_path}")
        logger.info(f"ğŸ“Š ì´ {len(videos)}ê°œ ë¹„ë””ì˜¤ (ìˆ˜ì§‘ì¼: {metadata.get('collected_at', 'N/A')})")
        
        return videos
    
    async def analyze_batch(
        self, 
        videos: List[Dict], 
        batch_size: int = 5,
        max_videos: Optional[int] = None
    ) -> List[Dict]:
        """ë¹„ë””ì˜¤ ë°°ì¹˜ ë¶„ì„"""
        
        if max_videos:
            videos = videos[:max_videos]
            logger.info(f"ğŸ¯ ë¶„ì„ ëŒ€ìƒì„ {max_videos}ê°œë¡œ ì œí•œ")
        
        logger.info(f"ğŸµ ì´ {len(videos)}ê°œ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë”©...")
        model_success = await audio_processor.initialize_models()
        if not model_success:
            logger.error("âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            return []
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for i in range(0, len(videos), batch_size):
            batch = videos[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(videos) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            tasks = []
            for video in batch:
                task = self.analyze_single_video(video)
                tasks.append(task)
            
            # ë°°ì¹˜ ì‹¤í–‰
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ë°°ì¹˜ ì˜¤ë¥˜: {batch[j]['title']}: {result}")
                    self.failed_analyses.append({
                        "video": batch[j],
                        "error": str(result),
                        "failed_at": datetime.now().isoformat()
                    })
                elif result:
                    self.analysis_results.append(result)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            success_count = len(self.analysis_results)
            fail_count = len(self.failed_analyses)
            processed = success_count + fail_count
            
            logger.info(f"ğŸ“Š ë°°ì¹˜ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {fail_count} (ì „ì²´ {processed}/{len(videos)})")
            
            # ë°°ì¹˜ ê°„ íœ´ì‹
            if i + batch_size < len(videos):
                await asyncio.sleep(2)
        
        # ìµœì¢… ì •ë¦¬
        audio_processor.cleanup_temp_files()
        
        logger.info(f"""
ğŸ‰ ì „ì²´ ë¶„ì„ ì™„ë£Œ!
   âœ… ì„±ê³µ: {len(self.analysis_results)}ê°œ
   âŒ ì‹¤íŒ¨: {len(self.failed_analyses)}ê°œ
   ğŸ“Š ì„±ê³µë¥ : {len(self.analysis_results) / len(videos) * 100:.1f}%
""")
        
        return self.analysis_results
    
    async def analyze_single_video(self, video: Dict) -> Optional[Dict]:
        """ë‹¨ì¼ ë¹„ë””ì˜¤ ë¶„ì„"""
        
        video_id = video["id"]
        url = video["url"]
        title = video["title"]
        
        try:
            logger.info(f"ğŸµ ë¶„ì„ ì‹œì‘: {title}")
            
            # ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤í–‰
            result = await audio_processor.process_youtube_video(url, video_id)
            
            if result["status"] == "completed":
                # ì›ë³¸ ë©”íƒ€ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ ë³‘í•©
                analysis_data = {
                    # ì›ë³¸ ì •ë³´
                    "video_id": video_id,
                    "url": url,
                    "title": title,
                    "description": video.get("description", ""),
                    "channelTitle": video.get("channelTitle", ""),
                    "publishedAt": video.get("publishedAt", ""),
                    "views": video.get("views", 0),
                    "likes": video.get("likes", 0),
                    "comments": video.get("comments", 0),
                    "duration": video.get("duration", ""),
                    "search_query": video.get("search_query", ""),
                    
                    # ë¶„ì„ ê²°ê³¼
                    "music_summary": {
                        "bpm": result["audio_features"]["bpm"],
                        "key": result["audio_features"]["key"],
                        "mode": result["audio_features"]["mode"],
                        "mode_confidence": result["audio_features"]["mode_confidence"],
                        "pitch_confidence": result["audio_features"]["pitch_confidence"],
                        "duration_analyzed": result["audio_features"]["duration"],
                        "beats_count": result["audio_features"]["beats_count"],
                        "real_analysis": True
                    },
                    
                    "emotion_analysis": {
                        "dominant_emotion": result["emotion"]["dominant_emotion"],
                        "confidence": result["emotion"]["confidence"],
                        "analysis": result["emotion"]["analysis"],
                        "real_analysis": True
                    },
                    
                    "lyrics_analysis": {
                        "lyrics": result["lyrics"]["text"],
                        "language": result["lyrics"]["language"],
                        "segments_count": len(result["lyrics"]["segments"]),
                        "confidence": result["lyrics"]["confidence"],
                        "real_analysis": True
                    },
                    
                    # ë©”íƒ€ë°ì´í„°
                    "processing_time": result["processing_time"],
                    "analyzed_at": datetime.now().isoformat(),
                    "analysis_version": "1.0"
                }
                
                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {title} | BPM: {result['audio_features']['bpm']:.1f} | í‚¤: {result['audio_features']['key']} {result['audio_features']['mode']}")
                return analysis_data
            
            else:
                logger.warning(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {title} - {result.get('error', 'Unknown error')}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {title}: {e}")
            return None
    
    def save_results(self, filename: Optional[str] = None) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"analyzed_results_{timestamp}.json"
        
        output_path = self.output_dir / filename
        
        # ë©”íƒ€ë°ì´í„° í¬í•¨í•œ ìµœì¢… ë°ì´í„°
        output_data = {
            "metadata": {
                "analyzed_at": datetime.now().isoformat(),
                "total_analyzed": len(self.analysis_results),
                "total_failed": len(self.failed_analyses),
                "success_rate": len(self.analysis_results) / (len(self.analysis_results) + len(self.failed_analyses)) * 100 if self.analysis_results or self.failed_analyses else 0,
                "analysis_version": "1.0",
                "tools_used": ["yt-dlp", "whisper", "torchcrepe", "librosa"]
            },
            "videos": self.analysis_results,
            "failed_analyses": self.failed_analyses
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # í†µê³„ ì¶œë ¥
        if self.analysis_results:
            bpms = [v["music_summary"]["bpm"] for v in self.analysis_results]
            avg_bpm = sum(bpms) / len(bpms)
            
            emotions = [v["emotion_analysis"]["dominant_emotion"] for v in self.analysis_results]
            emotion_counts = {}
            for emotion in emotions:
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            most_common_emotion = max(emotion_counts.items(), key=lambda x: x[1])
            
            logger.info(f"""
ğŸ“ˆ ë¶„ì„ í†µê³„:
   í‰ê·  BPM: {avg_bpm:.1f}
   ê°€ì¥ ë§ì€ ê°ì •: {most_common_emotion[0]} ({most_common_emotion[1]}ê°œ)
   ì–¸ì–´ ë¶„í¬: {len([v for v in self.analysis_results if v['lyrics_analysis']['language'] == 'ko'])}ê°œ í•œêµ­ì–´
""")
        
        return str(output_path)
    
    def create_upload_script(self, analysis_file: str) -> str:
        """ì„œë²„ ì—…ë¡œë“œìš© ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        
        upload_script_template = '''#!/usr/bin/env python3
"""
ì„œë²„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (EC2ì—ì„œ ì‹¤í–‰)
Step 2ì—ì„œ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì„œë²„ DBì— ì €ì¥
"""

import json
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
import sys
sys.path.append('/opt/trendy-lyrics/current')

from app.db import get_session, init_db
from app.models import Video, Embedding, AnalysisSummary
from app.services.text_embed import embed_text
from app.services.embeddings import to_bytes
from sqlmodel import select

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def upload_analysis_results():
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì„œë²„ DBì— ì—…ë¡œë“œ"""
    
    # ë¶„ì„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    analysis_file = "ANALYSIS_FILE_PLACEHOLDER"
    
    if not Path(analysis_file).exists():
        logger.error(f"ë¶„ì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {analysis_file}")
        return False
    
    with open(analysis_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    videos = data.get("videos", [])
    metadata = data.get("metadata", {})
    
    logger.info(f"ğŸ“ ì—…ë¡œë“œí•  ë¶„ì„ ê²°ê³¼: {len(videos)}ê°œ")
    logger.info(f"ğŸ“Š ì„±ê³µë¥ : {metadata.get('success_rate', 0):.1f}%")
    
    # DB ì´ˆê¸°í™” ë° ì„¸ì…˜ ìƒì„±
    init_db()
    session = next(get_session())
    
    uploaded_count = 0
    
    try:
        for i, video_data in enumerate(videos, 1):
            video_id = video_data["video_id"]
            title = video_data["title"]
            
            logger.info(f"ğŸ“¤ ì—…ë¡œë“œ {i}/{len(videos)}: {title}")
            
            # Video ì—”í‹°í‹° ìƒì„±/ì—…ë°ì´íŠ¸
            existing = session.exec(select(Video).where(Video.video_id == video_id)).first()
            
            if existing:
                existing.title = title
                existing.url = video_data["url"]
                existing.view_count = video_data.get("views", 0)
                video = existing
                logger.debug("ê¸°ì¡´ ë¹„ë””ì˜¤ ì—…ë°ì´íŠ¸")
            else:
                video = Video(
                    video_id=video_id,
                    url=video_data["url"],
                    title=title,
                    view_count=video_data.get("views", 0)
                )
                session.add(video)
                logger.debug("ìƒˆ ë¹„ë””ì˜¤ ìƒì„±")
            
            session.flush()
            
            # ì„ë² ë”© ìƒì„± (ì„œë²„ì—ì„œ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)
            title_embedding = embed_text(title)
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ lyricsë¡œ í™œìš©
            search_query = video_data.get("search_query", "")
            lyrics_text = f"{title} {search_query}"
            lyrics_embedding = embed_text(lyrics_text)
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            session.query(Embedding).filter(Embedding.video_id == video.id).delete()
            session.query(AnalysisSummary).filter(AnalysisSummary.video_id == video.id).delete()
            
            # ìƒˆ ì„ë² ë”© ì¶”ê°€
            embeddings = [
                Embedding(
                    video_id=video.id,
                    kind="title",
                    vector=to_bytes(title_embedding)
                ),
                Embedding(
                    video_id=video.id,
                    kind="lyrics", 
                    vector=to_bytes(lyrics_embedding)
                )
            ]
            
            for emb in embeddings:
                session.add(emb)
            
            # ë¶„ì„ ìš”ì•½ ì¶”ê°€
            summaries = [
                AnalysisSummary(
                    video_id=video.id,
                    kind="music_summary",
                    data_json=json.dumps(video_data["music_summary"])
                ),
                AnalysisSummary(
                    video_id=video.id,
                    kind="audio_emotion",
                    data_json=json.dumps(video_data["emotion_analysis"])
                ),
                AnalysisSummary(
                    video_id=video.id,
                    kind="lyrics_analysis",
                    data_json=json.dumps(video_data["lyrics_analysis"])
                )
            ]
            
            for summary in summaries:
                session.add(summary)
            
            uploaded_count += 1
            
            # ì£¼ê¸°ì  ì»¤ë°‹ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if i % 10 == 0:
                session.commit()
                logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {i}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
        # ìµœì¢… ì»¤ë°‹
        session.commit()
        
        logger.info(f"""
ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ!
   âœ… ì„±ê³µ: {uploaded_count}ê°œ
   ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ
""")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        session.rollback()
        return False
    
    finally:
        session.close()

if __name__ == "__main__":
    success = upload_analysis_results()
    if success:
        logger.info("âœ… ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ!")
    else:
        logger.error("âŒ ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨!")
'''
        
        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ëª… ìƒì„±
        script_filename = f"upload_{Path(analysis_file).stem}_to_server.py"
        script_path = self.output_dir / script_filename
        
        # ì‹¤ì œ ë¶„ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì¹˜í™˜
        upload_script = upload_script_template.replace("ANALYSIS_FILE_PLACEHOLDER", analysis_file)
        
        with open(script_path, "w", encoding="utf-8") as f:
            f.write(upload_script)
        
        logger.info(f"ğŸ“‹ ì„œë²„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}")
        return str(script_path)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description="YouTube ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„")
    parser.add_argument("collection_file", help="Step 1ì—ì„œ ìƒì„±ëœ ìˆ˜ì§‘ íŒŒì¼")
    parser.add_argument("--max-videos", type=int, help="ë¶„ì„í•  ìµœëŒ€ ë¹„ë””ì˜¤ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--batch-size", type=int, default=3, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 3)")
    
    args = parser.parse_args()
    
    analyzer = YouTubeAnalyzer()
    
    try:
        # ìˆ˜ì§‘ íŒŒì¼ ë¡œë“œ
        videos = analyzer.load_collection(args.collection_file)
        
        if not videos:
            logger.error("âŒ ë¶„ì„í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ë¶„ì„ ì‹¤í–‰
        results = asyncio.run(analyzer.analyze_batch(
            videos=videos,
            batch_size=args.batch_size,
            max_videos=args.max_videos
        ))
        
        if results:
            # ê²°ê³¼ ì €ì¥
            analysis_file = analyzer.save_results()
            
            # ì„œë²„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
            upload_script = analyzer.create_upload_script(Path(analysis_file).name)
            
            logger.info(f"""
ğŸ¯ Step 2 ì™„ë£Œ!
   ğŸ“ ë¶„ì„ ê²°ê³¼: {analysis_file}
   ğŸ“‹ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸: {upload_script}
   
ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (ì„œë²„ì—ì„œ ì‹¤í–‰):
   scp {Path(analysis_file).name} {Path(upload_script).name} ec2-user@3.36.70.96:/opt/trendy-lyrics/current/
   ssh -i ~/Desktop/keypair/umc-hackathon.pem ec2-user@3.36.70.96
   cd /opt/trendy-lyrics/current && python {Path(upload_script).name}
""")
        
        else:
            logger.error("âŒ ë¶„ì„ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()