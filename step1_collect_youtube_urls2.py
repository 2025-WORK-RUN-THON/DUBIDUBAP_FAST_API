#!/usr/bin/env python3
"""
Step 1: YouTube í‚¤ì›Œë“œ ê¸°ë°˜ URL ìˆ˜ì§‘ ìë™í™”
ë¡œê³ ì†¡, CMì†¡, ê´‘ê³ ìŒì•… ë“±ì˜ í‚¤ì›Œë“œë¡œ YouTube Data API ê²€ìƒ‰ (3ë¶„ ì´ë‚´ ì˜ìƒë§Œ í•„í„°ë§)
"""

import os
import json
import logging
import time
import re ### <-- NEW/MODIFIED ###
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import requests

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class YouTubeCollector:
    """YouTube Data APIë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://www.googleapis.com/youtube/v3"
        self.collected_videos = []
        self.seen_video_ids = set()
        
        # ê²°ê³¼ ì €ì¥ í´ë”
        self.output_dir = Path("data/collected")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @staticmethod ### <-- NEW/MODIFIED ###
    def _parse_duration(duration_str: str) -> Optional[int]: ### <-- NEW/MODIFIED ###
        """ISO 8601 duration í˜•ì‹ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.""" ### <-- NEW/MODIFIED ###
        if not duration_str: ### <-- NEW/MODIFIED ###
            return None ### <-- NEW/MODIFIED ###
        
        # Regex to parse ISO 8601 duration format (PT#H#M#S) ### <-- NEW/MODIFIED ###
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str) ### <-- NEW/MODIFIED ###
        if not match: ### <-- NEW/MODIFIED ###
            return None ### <-- NEW/MODIFIED ###
        
        hours = int(match.group(1)) if match.group(1) else 0 ### <-- NEW/MODIFIED ###
        minutes = int(match.group(2)) if match.group(2) else 0 ### <-- NEW/MODIFIED ###
        seconds = int(match.group(3)) if match.group(3) else 0 ### <-- NEW/MODIFIED ###
        
        delta = timedelta(hours=hours, minutes=minutes, seconds=seconds) ### <-- NEW/MODIFIED ###
        return int(delta.total_seconds()) ### <-- NEW/MODIFIED ###
    
    def search_videos(
        self, 
        query: str, 
        max_results: int = 50,
        order: str = "relevance",
        published_after: Optional[str] = None
    ) -> List[Dict]:
        """YouTube ê²€ìƒ‰ API í˜¸ì¶œ"""
        
        url = f"{self.base_url}/search"
        params = {
            "part": "snippet",
            "q": query,
            "type": "video",
            "maxResults": min(max_results, 50),
            "order": order,
            "key": self.api_key,
            "regionCode": "KR",
            "relevanceLanguage": "ko"
        }
        
        if published_after:
            params["publishedAfter"] = published_after
        
        try:
            logger.info(f"ğŸ” ê²€ìƒ‰ì¤‘: '{query}' (ìµœëŒ€ {max_results}ê°œ)")
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            videos = []
            
            for item in data.get("items", []):
                video_id = item["id"]["videoId"]
                
                if video_id in self.seen_video_ids:
                    continue
                
                video_info = {
                    "id": video_id,
                    "url": f"https://www.youtube.com/watch?v={video_id}",
                    "title": item["snippet"]["title"],
                    "description": item["snippet"]["description"],
                    "channelTitle": item["snippet"]["channelTitle"],
                    "publishedAt": item["snippet"]["publishedAt"],
                    "thumbnails": item["snippet"]["thumbnails"],
                    "search_query": query,
                    "collected_at": datetime.now().isoformat()
                }
                
                videos.append(video_info)
                self.seen_video_ids.add(video_id)
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(videos)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
            return videos
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ '{query}': {e}")
            return []
    
    def get_video_statistics(self, video_ids: List[str]) -> Dict[str, Dict]:
        """ë¹„ë””ì˜¤ í†µê³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€, **ì˜ìƒ ê¸¸ì´** ë“±)"""
        
        if not video_ids:
            return {}
        
        url = f"{self.base_url}/videos"
        all_stats = {}
        
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            params = {
                "part": "statistics,contentDetails",
                "id": ",".join(batch_ids),
                "key": self.api_key
            }
            
            try:
                response = requests.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                for item in data.get("items", []):
                    video_id = item["id"]
                    stats = item.get("statistics", {})
                    content_details = item.get("contentDetails", {})
                    
                    all_stats[video_id] = {
                        "views": int(stats.get("viewCount", 0)),
                        "likes": int(stats.get("likeCount", 0)),
                        "comments": int(stats.get("commentCount", 0)),
                        "duration": content_details.get("duration", ""),
                        "definition": content_details.get("definition", "sd")
                    }
                
                time.sleep(0.1)
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ğŸ“Š í†µê³„ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_stats)}ê°œ")
        return all_stats
    
    def collect_by_keywords(
        self, 
        keywords: List[str], 
        target_count: int = 100,
        min_views: int = 1000,
        max_duration_sec: int = 180 ### <-- NEW/MODIFIED ###
    ) -> List[Dict]:
        """í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¹„ë””ì˜¤ ìˆ˜ì§‘"""
        
        logger.info(f"ğŸ¯ ëª©í‘œ: ì´ {target_count}ê°œ ìˆ˜ì§‘ (ìµœì†Œ ì¡°íšŒìˆ˜: {min_views:,}, ìµœëŒ€ ê¸¸ì´: {max_duration_sec}ì´ˆ)") ### <-- NEW/MODIFIED ###
        
        per_keyword = max(target_count // len(keywords), 10)
        
        for keyword in keywords:
            if len(self.collected_videos) >= target_count:
                break
            
            videos = self.search_videos(keyword, max_results=per_keyword * 2)
            
            if videos:
                video_ids = [v["id"] for v in videos]
                stats = self.get_video_statistics(video_ids)
                
                collected_count_for_keyword = 0 ### <-- NEW/MODIFIED ###
                for video in videos:
                    video_id = video["id"]
                    if video_id in stats:
                        video.update(stats[video_id])
                        
                        # ì¡°íšŒìˆ˜ í•„í„°
                        if video.get("views", 0) < min_views:
                            logger.debug(f"ğŸ“‰ ì¡°íšŒìˆ˜ ë¶€ì¡±: {video['title']} ({video.get('views', 0):,})")
                            continue
                        
                        # ### ì˜ìƒ ê¸¸ì´ í•„í„° ### <-- NEW/MODIFIED ###
                        duration_sec = self._parse_duration(video.get("duration", "")) ### <-- NEW/MODIFIED ###
                        if duration_sec is None or duration_sec > max_duration_sec: ### <-- NEW/MODIFIED ###
                            logger.debug(f"â³ ì˜ìƒ ê¸¸ì´ ì´ˆê³¼: {video['title']} ({duration_sec}ì´ˆ)") ### <-- NEW/MODIFIED ###
                            continue ### <-- NEW/MODIFIED ###
                        
                        # ëª¨ë“  í•„í„° í†µê³¼
                        video["duration_seconds"] = duration_sec ### <-- NEW/MODIFIED ###
                        self.collected_videos.append(video)
                        collected_count_for_keyword += 1 ### <-- NEW/MODIFIED ###

                logger.info(f"ğŸ’¾ '{keyword}' ìˆ˜ì§‘: {collected_count_for_keyword}ê°œ ì¶”ê°€") ### <-- NEW/MODIFIED ###
                
            time.sleep(1)
        
        self.collected_videos.sort(key=lambda x: x.get("views", 0), reverse=True)
        
        if len(self.collected_videos) > target_count:
            self.collected_videos = self.collected_videos[:target_count]
        
        logger.info(f"ğŸ‰ ìµœì¢… ìˆ˜ì§‘: {len(self.collected_videos)}ê°œ")
        return self.collected_videos
    
    def save_results(self, filename: Optional[str] = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_collection_{timestamp}.json"
        
        output_path = self.output_dir / filename
        
        output_data = {
            "metadata": {
                "collected_at": datetime.now().isoformat(),
                "total_count": len(self.collected_videos),
                "collection_method": "youtube_data_api_v3",
                "keywords_used": list(set(v.get("search_query") for v in self.collected_videos)),
                "min_views_filter": True,
                "max_duration_filter_sec": 180 ### <-- NEW/MODIFIED ###
            },
            "videos": self.collected_videos
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        if not self.collected_videos: ### <-- NEW/MODIFIED ###
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ê°€ ì—†ì–´ í†µê³„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") ### <-- NEW/MODIFIED ###
            return str(output_path) ### <-- NEW/MODIFIED ###

        total_views = sum(v.get("views", 0) for v in self.collected_videos)
        avg_views = total_views // len(self.collected_videos) if self.collected_videos else 0
        
        logger.info(f"""
ğŸ“ˆ ìˆ˜ì§‘ í†µê³„:
   ì´ ë¹„ë””ì˜¤: {len(self.collected_videos):,}ê°œ
   ì´ ì¡°íšŒìˆ˜: {total_views:,}
   í‰ê·  ì¡°íšŒìˆ˜: {avg_views:,}
   ìµœê³  ì¡°íšŒìˆ˜: {max(v.get('views', 0) for v in self.collected_videos):,}
""")
        
        return str(output_path)
    
    def validate_collection(self) -> Dict:
        """ìˆ˜ì§‘ ê²°ê³¼ ê²€ì¦"""
        
        if not self.collected_videos:
            return {"valid": False, "errors": ["ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤"]}
        
        errors = []
        warnings = []
        
        urls = [v["url"] for v in self.collected_videos]
        if len(urls) != len(set(urls)):
            errors.append("ì¤‘ë³µëœ URLì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        low_views = [v for v in self.collected_videos if v.get("views", 0) < 1000]
        if len(low_views) > len(self.collected_videos) * 0.3:
            warnings.append(f"ì¡°íšŒìˆ˜ê°€ ë‚®ì€ ë¹„ë””ì˜¤ê°€ ë§ìŠµë‹ˆë‹¤: {len(low_views)}ê°œ")
        
        old_videos = []
        one_year_ago = datetime.now() - timedelta(days=365)
        
        for video in self.collected_videos:
            try:
                published = datetime.fromisoformat(video["publishedAt"].replace("Z", "+00:00"))
                if published < one_year_ago:
                    old_videos.append(video)
            except:
                pass
        
        if len(old_videos) > len(self.collected_videos) * 0.5:
            warnings.append(f"1ë…„ ì´ìƒ ëœ ë¹„ë””ì˜¤ê°€ ë§ìŠµë‹ˆë‹¤: {len(old_videos)}ê°œ")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "stats": {
                "total": len(self.collected_videos),
                "low_views": len(low_views),
                "old_videos": len(old_videos)
            }
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        logger.error("âŒ YOUTUBE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        logger.error("   Google Cloud Consoleì—ì„œ YouTube Data API v3 í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”")
        logger.error("   export YOUTUBE_API_KEY='your_api_key_here'")
        return
    
    keywords = [
        "ë¡œê³ ì†¡", "CMì†¡", "ê´‘ê³ ìŒì•…", "ë¸Œëœë“œì†¡", "ê¸°ì—… ìŒì•…",
        "ê´‘ê³  BGM", "ìƒì—… ìŒì•…", "ë§ˆì¼€íŒ… ìŒì•…", "ë¸Œëœë“œ ë®¤ì§",
        "ê¸°ì—… ë¡œê³ ì†¡", "TV ê´‘ê³  ìŒì•…", "ë¼ë””ì˜¤ ê´‘ê³ ",
        "YouTube ê´‘ê³  ìŒì•…", "ë¸Œëœë”© ìŒì•…", "ì œí’ˆ ê´‘ê³  ìŒì•…"
    ]
    
    collector = YouTubeCollector(api_key)
    
    try:
        videos = collector.collect_by_keywords(
            keywords=keywords,
            target_count=100,
            min_views=5000,
            max_duration_sec=180  # 3ë¶„ (3 * 60 = 180ì´ˆ) ### <-- NEW/MODIFIED ###
        )
        
        if videos:
            output_file = collector.save_results("collected_urls_100.json")
            validation = collector.validate_collection()
            
            if validation["valid"]:
                logger.info("âœ… ìˆ˜ì§‘ ê²€ì¦ í†µê³¼!")
            else:
                logger.warning("âš ï¸ ê²€ì¦ ì˜¤ë¥˜:")
                for error in validation["errors"]:
                    logger.error(f"   {error}")
            
            if validation["warnings"]:
                logger.warning("âš ï¸ ê²½ê³ ì‚¬í•­:")
                for warning in validation["warnings"]:
                    logger.warning(f"   {warning}")
            
            logger.info(f"""
ğŸ¯ Step 1 ì™„ë£Œ!
   ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}
   ğŸ“Š ìˆ˜ì§‘ ê°œìˆ˜: {len(videos)}ê°œ
   
ğŸš€ ë‹¤ìŒ ë‹¨ê³„: 
   python step2_download_and_analyze.py {Path(output_file).name}
""")
            
        else:
            logger.error("âŒ ì¡°ê±´ì— ë§ëŠ” ë¹„ë””ì˜¤ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)


if __name__ == "__main__":
    main()