"""
ì‹¤ì œ YouTube ìŒì•… ë¶„ì„ ì‹œìŠ¤í…œ
ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ â†’ ê°€ì‚¬ ì¶”ì¶œ â†’ ê°ì„± ë¶„ì„ â†’ ìŒí–¥ íŠ¹ì„± ë¶„ì„
"""

import os
import logging
import tempfile
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json

import yt_dlp
import whisper
import librosa
import numpy as np
import torchcrepe

logger = logging.getLogger(__name__)


class AudioProcessor:
    """YouTube ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.whisper_model = None
        self.audio_temp_dir = Path("temp_audio")
        self.audio_temp_dir.mkdir(exist_ok=True)
        
    async def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        try:
            # Whisper ëª¨ë¸ ë¡œë“œ (base ëª¨ë¸ ì‚¬ìš©)
            logger.info("Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.whisper_model = whisper.load_model("base")
            
            logger.info("âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def download_audio(self, youtube_url: str) -> Optional[str]:
        """YouTubeì—ì„œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"""
        try:
            output_path = self.audio_temp_dir / f"audio_{hash(youtube_url)}.wav"
            
            ydl_opts = {
                'format': 'bestaudio/best',
                'outtmpl': str(output_path.with_suffix('.%(ext)s')),
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'wav',
                    'preferredquality': '192',
                }],
                'quiet': True,
                'no_warnings': True
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([youtube_url])
            
            if output_path.exists():
                logger.info(f"âœ… ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
                return str(output_path)
            else:
                logger.error(f"ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {output_path}")
                return None
                
        except Exception as e:
            logger.error(f"YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {youtube_url}: {e}")
            return None
    
    async def extract_lyrics(self, audio_path: str) -> Dict:
        """Whisperë¡œ ê°€ì‚¬ ì¶”ì¶œ"""
        try:
            if not self.whisper_model:
                await self.initialize_models()
            
            logger.info(f"ê°€ì‚¬ ì¶”ì¶œ ì¤‘: {audio_path}")
            result = self.whisper_model.transcribe(audio_path, language="ko")
            
            lyrics_data = {
                "text": result["text"].strip(),
                "segments": [
                    {
                        "start": seg["start"],
                        "end": seg["end"], 
                        "text": seg["text"].strip()
                    }
                    for seg in result["segments"]
                ],
                "language": result["language"],
                "confidence": np.mean([seg.get("no_speech_prob", 0) for seg in result["segments"]])
            }
            
            logger.info(f"âœ… ê°€ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {len(lyrics_data['text'])}ì")
            return lyrics_data
            
        except Exception as e:
            logger.error(f"ê°€ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"text": "", "segments": [], "language": "unknown", "confidence": 0.0}
    
    async def analyze_audio_emotion(self, audio_path: str) -> Dict:
        """ìŒí–¥ íŠ¹ì„± ê¸°ë°˜ ê°„ë‹¨í•œ ê°ì„± ë¶„ì„"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=22050)
            
            # ê¸°ë³¸ ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
            zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))
            
            # ê°„ë‹¨í•œ ê°ì„± ì¶”ì • (íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)
            if tempo > 140 and spectral_centroid > 2000:
                emotion = "energetic"
                confidence = 0.8
            elif tempo < 90 and spectral_centroid < 1500:
                emotion = "calm"
                confidence = 0.7
            elif tempo > 120 and zero_crossing_rate > 0.1:
                emotion = "happy"
                confidence = 0.75
            else:
                emotion = "neutral"
                confidence = 0.6
            
            return {
                "dominant_emotion": emotion,
                "confidence": confidence,
                "analysis": {
                    "tempo": float(tempo),
                    "spectral_centroid": float(spectral_centroid),
                    "zero_crossing_rate": float(zero_crossing_rate)
                }
            }
            
        except Exception as e:
            logger.error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"dominant_emotion": "neutral", "confidence": 0.5, "analysis": {}}
    
    async def analyze_audio_features(self, audio_path: str) -> Dict:
        """ìŒí–¥ íŠ¹ì„± ë¶„ì„ (BPM, í‚¤, ëª¨ë“œ ë“±) - TorchCrepe ì‚¬ìš©"""
        try:
            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=22050)
            
            # BPM ì¶”ì •
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # TorchCrepeë¡œ ìŒì • ì¶”ì •
            try:
                # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (TorchCrepe ê¶Œì¥)
                y_16k = librosa.resample(y, orig_sr=sr, target_sr=16000)
                
                # ìŒì • ì¶”ì  (f0 ì¶”ì •)
                frequency, confidence = torchcrepe.predict(
                    y_16k, 
                    16000, 
                    hop_length=160,
                    fmin=50, 
                    fmax=550,
                    model='tiny',
                    batch_size=512
                )
                
                # ìœ íš¨í•œ ìŒì •ë§Œ í•„í„°ë§ (ì‹ ë¢°ë„ 0.5 ì´ìƒ)
                valid_freqs = frequency[confidence > 0.5]
                
                if len(valid_freqs) > 0:
                    # ì£¼ìš” ìŒì • ì¶”ì •
                    median_freq = np.median(valid_freqs)
                    
                    # ìŒì •ì„ ë…¸íŠ¸ë¡œ ë³€í™˜
                    A4_freq = 440.0
                    note_number = 12 * np.log2(median_freq / A4_freq) + 69
                    note_index = int(round(note_number)) % 12
                    
                    key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
                    estimated_key = key_names[note_index]
                    pitch_confidence = float(np.mean(confidence[confidence > 0.5]))
                else:
                    # í´ë°±: í¬ë¡œë§ˆ ê¸°ë°˜ í‚¤ ì¶”ì •
                    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
                    chroma_mean = np.mean(chroma, axis=1)
                    key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
                    estimated_key = key_names[np.argmax(chroma_mean)]
                    pitch_confidence = 0.6
                    
            except Exception as crepe_error:
                logger.warning(f"TorchCrepe ë¶„ì„ ì‹¤íŒ¨, í¬ë¡œë§ˆë¡œ í´ë°±: {crepe_error}")
                # í´ë°±: í¬ë¡œë§ˆ ê¸°ë°˜ í‚¤ ì¶”ì •
                chroma = librosa.feature.chroma_stft(y=y, sr=sr)
                chroma_mean = np.mean(chroma, axis=1)
                key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
                estimated_key = key_names[np.argmax(chroma_mean)]
                pitch_confidence = 0.5
            
            # ê°„ë‹¨í•œ ëª¨ë“œ ì¶”ì • (ì¥ì¡°/ë‹¨ì¡°)
            if tempo > 120:
                mode = "major"
                mode_confidence = 0.7
            else:
                mode = "minor" 
                mode_confidence = 0.6
            
            return {
                "bpm": float(tempo),
                "key": estimated_key,
                "mode": mode,
                "mode_confidence": float(mode_confidence),
                "pitch_confidence": pitch_confidence,
                "duration": float(len(y) / sr),
                "beats_count": len(beats)
            }
            
        except Exception as e:
            logger.error(f"ìŒí–¥ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "bpm": 120.0,
                "key": "C",
                "mode": "major",
                "mode_confidence": 0.5,
                "pitch_confidence": 0.5,
                "duration": 0.0,
                "beats_count": 0
            }
    
    async def process_youtube_video(self, youtube_url: str, video_id: str) -> Dict:
        """YouTube ë¹„ë””ì˜¤ ì™„ì „ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"ğŸµ YouTube ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_id}")
        
        results = {
            "video_id": video_id,
            "url": youtube_url,
            "status": "processing",
            "lyrics": {},
            "emotion": {},
            "audio_features": {},
            "processing_time": 0
        }
        
        import time
        start_time = time.time()
        
        try:
            # 1. ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
            logger.info("1ï¸âƒ£ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            audio_path = await self.download_audio(youtube_url)
            if not audio_path:
                results["status"] = "download_failed"
                return results
            
            # 2. ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¶„ì„ ìˆ˜í–‰
            logger.info("2ï¸âƒ£ ë³‘ë ¬ ë¶„ì„ ì‹œì‘...")
            tasks = [
                self.extract_lyrics(audio_path),
                self.analyze_audio_emotion(audio_path),
                self.analyze_audio_features(audio_path)
            ]
            
            lyrics_data, emotion_data, features_data = await asyncio.gather(*tasks)
            
            results.update({
                "status": "completed",
                "lyrics": lyrics_data,
                "emotion": emotion_data,
                "audio_features": features_data,
                "processing_time": time.time() - start_time
            })
            
            # 3. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(audio_path):
                os.remove(audio_path)
            
            logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {video_id} ({results['processing_time']:.2f}ì´ˆ)")
            return results
            
        except Exception as e:
            logger.error(f"ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨ {video_id}: {e}")
            results.update({
                "status": "failed",
                "error": str(e),
                "processing_time": time.time() - start_time
            })
            return results
    
    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            for file in self.audio_temp_dir.glob("*"):
                file.unlink()
            logger.info("ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
audio_processor = AudioProcessor()